# FootyGPT

This repository contains an implementation of a transformer-based GPT (Generative Pre-trained Transformer) language model using PyTorch. The model is trained on text data and is capable of generating text based on a given prompt.

Features

Implements a Transformer-based language model similar to GPT.

Uses self-attention mechanisms and multi-head attention for text processing.

Includes an embedding layer for token and positional encoding.

Supports text tokenization and encoding/decoding functions.

Implements training with an AdamW optimizer and cross-entropy loss.

Allows text generation using the trained model.
